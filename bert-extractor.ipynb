{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Key Phrase Extractor\n",
    "\n",
    "In this notebook we aim to realize the Bottom-Up Summarization Paper's extractor with BERT as the contextual embedding and see if we are able to extract phrases that maximizes the ROGUE scores. Our first goal in this project is to generate non-sensical summaries that maximizes the ROGUE score. Then, we aim to train an additional language model-like network to generate abstractive summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "\n",
    "from rouge import Rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_STORY_DIR = os.path.join('data', 'cnn', 'stories')\n",
    "DM_STORY_DIR = os.path.join('data', 'dailymail', 'stories')\n",
    "\n",
    "CNN_STORY_TOKENIZED = os.path.join('data', 'cnn', 'stories-tokenized')\n",
    "DM_STORY_TOKENIZED = os.path.join('data', 'dailymail', 'stories-tokenized')\n",
    "\n",
    "SRC_JSON = os.path.join('data', 'src.pk')\n",
    "TGT_JSON = os.path.join('data', 'tgt.pk')\n",
    "\n",
    "PREDICTED_SUMS = os.path.join('out', 'predicted')\n",
    "GOLD_SUMS = os.path.join('out', 'gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We will first read in the files and process them into tokenized sentences and words, and separate out the source document and the abstract. Here, we heavily borrowed code from Pointer Generator code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [d for d in os.listdir(CNN_STORY_TOKENIZED)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_single_close_quote = u'\\u2019' # unicode\n",
    "dm_double_close_quote = u'\\u201d'\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "def process_json(filename):\n",
    "    src, tgt = [], [] # a document is a list of list of words\n",
    "    highlight = False # highlights are always at the end of the document \n",
    "    f = open(filename, 'r')\n",
    "    parsed = json.load(f)\n",
    "    for sent in parsed['sentences']:\n",
    "        words = [word['word'] for word in sent['tokens']]\n",
    "        if words[-1] not in END_TOKENS:\n",
    "            words += ['.']\n",
    "        if words[0] == '@highlight':\n",
    "            highlight = True\n",
    "        elif highlight:\n",
    "            tgt += [words]\n",
    "        else:\n",
    "            src += [words]\n",
    "    return src, tgt\n",
    "\n",
    "src, tgt = process_json(os.path.join(CNN_STORY_TOKENIZED, dirs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7301587301587301"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def percentage_in_src_vocab(src, tgt):\n",
    "    src_vocab = set()\n",
    "    for sent in src:\n",
    "        src_vocab |= set(sent)\n",
    "    count = 0\n",
    "    total_len = 0\n",
    "    for sent in tgt:\n",
    "        for word in sent:\n",
    "            if word in src_vocab:\n",
    "                count += 1\n",
    "            total_len += 1\n",
    "    return count / total_len\n",
    "percentage_in_src_vocab(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_json(file_dir):\n",
    "    pool = Pool(processes=10)\n",
    "    srcs, tgts = [], []\n",
    "    percentages = []\n",
    "    file_paths = [os.path.join(file_dir, file_name) for file_name in os.listdir(file_dir)]\n",
    "    for tup in pool.imap_unordered(process_json, file_paths):\n",
    "        src, tgt = tup\n",
    "        srcs.append(src)\n",
    "        tgts.append(tgt)\n",
    "        percentages.append(percentage_in_src_vocab(src, tgt))\n",
    "    print(np.mean(percentages))\n",
    "    return srcs, tgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8373284430595709\n"
     ]
    }
   ],
   "source": [
    "srcs_cnn, tgts_cnn = process_all_json(CNN_STORY_TOKENIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8751616451112775\n"
     ]
    }
   ],
   "source": [
    "srcs_dm, tgts_dm = process_all_json(DM_STORY_TOKENIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt = srcs_cnn + srcs_dm, tgts_cnn + tgts_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(SRC_JSON, 'wb')\n",
    "pickle.dump(src, f)\n",
    "f.close()\n",
    "\n",
    "f = open(TGT_JSON, 'wb')\n",
    "pickle.dump(tgt, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(SRC_JSON, 'rb')\n",
    "src = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(TGT_JSON, 'rb')\n",
    "tgt = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To use BERT, we must format our data into one that BERT is able to use. We also have to redefine the problem as a sequence tagging problem presented in the Bottom-Up paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(doc, tgt):\n",
    "    \"\"\"\n",
    "    doc: a list of src tokens\n",
    "    tgt: a list of tgt tokens that we will look for in the doc\n",
    "    \"\"\"\n",
    "    if len(tgt) == 0:\n",
    "        print('zero sized tgt')\n",
    "        return None\n",
    "    label = np.zeros(len(doc), dtype=int)\n",
    "    l, r = 0, 0\n",
    "    while r < len(tgt):\n",
    "        old_idxs = []\n",
    "        idxs = [(i,i+1) for i, token in enumerate(doc) if token == tgt[r]]\n",
    "        while len(idxs) > 0 and r + 1 < len(tgt):\n",
    "            r += 1\n",
    "            old_idxs, idxs = idxs, []\n",
    "            for idx in old_idxs:\n",
    "                if doc[idx[-1]] == tgt[r]:\n",
    "                    idxs.append((idx[0], idx[-1] + 1))\n",
    "        if len(idxs) > 0: ## we ran out of tgt\n",
    "            label[idxs[0][0]:idxs[0][-1]] = 1\n",
    "            break\n",
    "        elif len(old_idxs) > 0: ## we found longest seq\n",
    "            label[old_idxs[0][0]:old_idxs[0][-1]] = 1\n",
    "        else: ## this token does not exist\n",
    "            r += 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_src_tgt(srcs, tgts, start_idx=0, end_idx=-1):\n",
    "    assert len(srcs) == len(tgts)\n",
    "    docs, tags = [], []\n",
    "    tagged_sum, gold_sum = [], []\n",
    "    rn = range(len(srcs)) if end_idx == -1 else range(start_idx, end_idx)\n",
    "    for i in rn:\n",
    "        ## process src\n",
    "        sents = [' '.join(sent) + ' [SEP]' for sent in srcs[i]]\n",
    "        doc = ' '.join(['[CLS]'] + sents)\n",
    "        doc = tokenizer.tokenize(doc)[:510] + ['[SEP]']\n",
    "        \n",
    "        ## process tgt\n",
    "        tgt = ' '.join([' '.join(sent) for sent in tgts[i]])\n",
    "        tgt = tokenizer.tokenize(tgt)[:110]\n",
    "        label = tag(doc, tgt)\n",
    "        \n",
    "        ## Add both to list\n",
    "        docs.append(tokenizer.convert_tokens_to_ids(doc))\n",
    "        tags.append(label)\n",
    "        tagged_sum.append((' '.join(np.array(doc)[label.astype(bool)])).replace(' ##', ''))\n",
    "        gold_sum.append(' '.join(tgt).replace(' ##', ''))\n",
    "    return docs, tags, tagged_sum, gold_sum\n",
    "docs, tags, tagged_sums, gold_sums = process_src_tgt(src, tgt, 1000, 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ranges(args):\n",
    "    return process_src_tgt(src, tgt, args[0], args[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 40\n",
    "pool = Pool(n)\n",
    "k = len(src)//n\n",
    "result = pool.map(process_ranges, [(start * k, (start+1) * k) for start in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, tags, tagged_sums, gold_sums = [], [], [], []\n",
    "for a, b, c, d in result:\n",
    "    docs.extend(a)\n",
    "    tags.extend(b)\n",
    "    tagged_sums.extend(c)\n",
    "    gold_sums.extend(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_cleaned, tags_cleaned, tagged_sums_cleaned, gold_sums_cleaned = [], [], [], []\n",
    "for i in range(len(docs)):\n",
    "    if len(tagged_sums[i]) > 0 and len(gold_sums[i]) > 0:\n",
    "        docs_cleaned.append(docs[i])\n",
    "        tags_cleaned.append(tags[i])\n",
    "        tagged_sums_cleaned.append(tagged_sums[i])\n",
    "        gold_sums_cleaned.append(gold_sums[i])\n",
    "docs, tags, tagged_sums, gold_sums = docs_cleaned, tags_cleaned, tagged_sums_cleaned, gold_sums_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = os.path.join('data', 'docs.pk')\n",
    "TAGS = os.path.join('data', 'tags.pk')\n",
    "TAGGED_SUMS = os.path.join('data', 'tagged_sums.pk')\n",
    "GOLD_SUMS = os.path.join('data', 'gold_sums.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path, obj in zip([DOCS, TAGS, TAGGED_SUMS, GOLD_SUMS], [docs, tags, tagged_sums, gold_sums]):\n",
    "    f = open(file_path, 'wb')\n",
    "    pickle.dump(obj, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\", for . jonas cuaron son of mexican director alfonso cuaron the a feature film and is a montage of stills have , ` ` ano una ' ' ' s father\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sums[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"jonas cuaron ' s debut feature , ` ` ano una ' ' is a montage of photographic stills . jonas is the son of mexican new wave director , alfonso cuaron . father and son have launched a short film competition for young filmmakers .\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_sums[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.8792832549923678, 'p': 0.9920658394023293, 'r': 0.798113827257415}, 'rouge-2': {'f': 0.5025017957813821, 'p': 0.5673329261939167, 'r': 0.4560521901503767}, 'rouge-l': {'f': 0.7111830800695053, 'p': 0.82231380861409, 'r': 0.6666003987211477}}\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(tagged_sums, gold_sums, avg=True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Model\n",
    "\n",
    "We have calculated the \"oracle\" score above, and now we would like to fit a model that accurately predicts the tags defined above.\n",
    "\n",
    "Later, we might change how the tags are defined and see if we can achieve better results than \"first occurance tagging\"\n",
    "\n",
    "We will split 90/5/5 with a 5k tiny dataset selected from the train set for faster development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407873900/407873900 [00:38<00:00, 10586104.93B/s]\n"
     ]
    }
   ],
   "source": [
    "model = ertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "tokens_tensor = torch.tensor([doc])\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor[0], output_all_encoded_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0216,  0.2807, -0.0808,  ...,  0.0314,  0.6625,  0.6284],\n",
       "        [ 0.9894,  0.4365,  0.1380,  ...,  0.8988,  1.3305,  0.3496],\n",
       "        [ 0.2269, -0.3134,  0.4247,  ..., -0.3216,  0.7174,  0.7578],\n",
       "        ...,\n",
       "        [ 0.5443, -0.2824, -0.1584,  ..., -0.1154,  0.3110,  0.3094],\n",
       "        [ 0.6949,  0.4653, -0.3678,  ...,  0.0592, -0.4090, -0.4708],\n",
       "        [ 0.4271,  0.3990,  0.0077,  ...,  0.2882, -0.3808, -0.4419]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_srcs = process_src(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_srcs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
