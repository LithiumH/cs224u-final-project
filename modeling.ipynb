{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Here we attempt to quickly mock up some models and transfer them into a python file for a long running query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import logging\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torch.utils.data as data\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rouge import Rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_STORY_DIR = os.path.join('data', 'cnn', 'stories')\n",
    "DM_STORY_DIR = os.path.join('data', 'dailymail', 'stories')\n",
    "\n",
    "CNN_STORY_TOKENIZED = os.path.join('data', 'cnn', 'stories-tokenized')\n",
    "DM_STORY_TOKENIZED = os.path.join('data', 'dailymail', 'stories-tokenized')\n",
    "\n",
    "SRC_JSON = os.path.join('data', 'src.pk')\n",
    "TGT_JSON = os.path.join('data', 'tgt.pk')\n",
    "\n",
    "PREDICTED_SUMS = os.path.join('out', 'predicted')\n",
    "GOLD_SUMS = os.path.join('out', 'gold')\n",
    "PROCESSED_DATA = os.path.join('data', 'data.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSED_DATA, 'rb') as f:\n",
    "    all_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Here we define some torch models \n",
    "\n",
    "1. Linear model - After BERT, we have a single linear layer that transforms the last hidden layer output for each timestamp into a single logit\n",
    "2. Linear model with multi-head attention - After BERT, we calculate the attention distribution and reweigh each hidden state before feeding it into a linear layer as in the previous model\n",
    "3. Linear model with multi-layer attention - Same as before but attend to ALL hidden layers\n",
    "4. RNN decoder with attention - After BERT, have another stacked bi-directional layer with attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_devices():\n",
    "    \"\"\"Get IDs of all available GPUs.\n",
    "\n",
    "    Returns:\n",
    "        device (torch.device): Main device (GPU 0 or CPU).\n",
    "        gpu_ids (list): List of IDs of all GPUs that are available.\n",
    "    \"\"\"\n",
    "    gpu_ids = []\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_ids += [gpu_id for gpu_id in range(torch.cuda.device_count())]\n",
    "        device = torch.device('cuda:{}'.format(gpu_ids[0]))\n",
    "        torch.cuda.set_device(device)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    return device, gpu_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-seed\", default=1234)\n",
    "parser.add_argument(\"-load_path\", default=None)\n",
    "parser.add_argument(\"-split\", default='tiny')\n",
    "parser.add_argument(\"-batch_size\", default=1)\n",
    "parser.add_argument(\"-gpu_ids\", default=[0])\n",
    "parser.add_argument(\"-num_workers\", default=1)\n",
    "parser.add_argument(\"-lr\", default=0.001)\n",
    "parser.add_argument(\"-l2_wd\", default=0)\n",
    "parser.add_argument(\"-eval_steps\", default=50000)\n",
    "parser.add_argument(\"-num_epochs\", default=10)\n",
    "parser.add_argument(\"-max_grad_norm\", default=2)\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(SummarizationDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __getitem__(self, i):\n",
    "        return (self.X[i], self.y[i])\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    collate function requires all examples to be non-padded\n",
    "    \"\"\"\n",
    "    def merge_1d(arrays, dtype=torch.int64, pad_value=0):\n",
    "        lengths = [len(a) for a in arrays]\n",
    "        padded = torch.zeros(len(arrays), max(lengths), dtype=dtype)\n",
    "        for i, seq in enumerate(arrays):\n",
    "            end = lengths[i]\n",
    "            padded[i, :end] = torch.tensor(seq)[:end]\n",
    "        return padded\n",
    "    X, y = zip(*examples)\n",
    "    return merge_1d(X), merge_1d(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_HIDDEN_SIZE = 768\n",
    "class SummarizerLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SummarizerLinear, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.linear = nn.Linear(BERT_HIDDEN_SIZE, 1)\n",
    "        xavier_uniform_(self.linear.weight)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        encoded_layers, _ = self.bert(X, output_all_encoded_layers=False)\n",
    "        enc = self.linear(encoded_layers[0]).transpose(0, 1) ## 1D array\n",
    "        return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger('main')\n",
    "log.setLevel(logging.DEBUG)\n",
    "def train(args):\n",
    "    log.info(\"\")\n",
    "#     device, args.gpu_ids = get_available_devices()\n",
    "    device, args.gpu_ids = torch.device('cpu'), []\n",
    "    \n",
    "    # Set random seed\n",
    "    log.info('Using random seed {}...'.format(args.seed))\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    \n",
    "    log.info('Building model...')\n",
    "    model = SummarizerLinear()\n",
    "#     model = nn.DataParallel(model, args.gpu_ids)\n",
    "    if args.load_path:\n",
    "        log.info('Loading checkpoint from {}...'.format(args.load_path))\n",
    "        model, step = util.load_model(model, args.load_path, args.gpu_ids)\n",
    "    else:\n",
    "        step = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), args.lr,\n",
    "                               weight_decay=args.l2_wd)\n",
    "    \n",
    "    log.info('Building dataset...')\n",
    "    train_dataset = SummarizationDataset(all_data['tiny']['X'], all_data['tiny']['y'])\n",
    "    dev_dataset = SummarizationDataset(all_data['tiny']['X'], all_data['tiny']['y'])\n",
    "    train_loader = data.DataLoader(train_dataset, \n",
    "                                   batch_size=args.batch_size,\n",
    "                                   num_workers=args.num_workers,\n",
    "                                   shuffle=True,\n",
    "                                   collate_fn=collate_fn)\n",
    "    dev_loader = data.DataLoader(dev_dataset, \n",
    "                                   batch_size=args.batch_size,\n",
    "                                   num_workers=args.num_workers,\n",
    "                                   shuffle=False,\n",
    "                                   collate_fn=collate_fn)\n",
    "    ## Train!\n",
    "    log.info('Training...')\n",
    "    steps_till_eval = args.eval_steps\n",
    "    epoch = step // len(train_dataset)\n",
    "    while epoch != args.num_epochs:\n",
    "        epoch += 1\n",
    "        log.info('Starting epoch {}...'.format(epoch))\n",
    "        with torch.enable_grad(), \\\n",
    "                tqdm(total=len(train_loader.dataset)) as progress_bar:\n",
    "            for X, y in train_loader:\n",
    "                X = X.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                logits = model(X)\n",
    "                y = y.float().to(device)\n",
    "                loss = F.binary_cross_entropy_with_logits(logits, y)\n",
    "                loss_val = loss.item()\n",
    "                \n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                # scheduler.step(step // batch_size)\n",
    "                \n",
    "                # Log info\n",
    "                step += args.batch_size\n",
    "                progress_bar.update(args.batch_size)\n",
    "                progress_bar.set_postfix(epoch=epoch,\n",
    "                                         NLL=loss_val)\n",
    "#                 tbx.add_scalar('train/NLL', loss_val, step)\n",
    "#                 tbx.add_scalar('train/LR',\n",
    "#                                optimizer.param_groups[0]['lr'],\n",
    "#                                step)\n",
    "\n",
    "#                 steps_till_eval -= batch_size\n",
    "#                 if steps_till_eval <= 0:\n",
    "#                     steps_till_eval = args.eval_steps\n",
    "\n",
    "#                     # Evaluate and save checkpoint\n",
    "#                     log.info('Evaluating at step {}...'.format(step))\n",
    "#                     ema.assign(model)\n",
    "#                     results, pred_dict = evaluate(model, dev_loader, device,\n",
    "#                                                   args.dev_eval_file,\n",
    "#                                                   args.max_ans_len,\n",
    "#                                                   args.use_squad_v2)\n",
    "#                     saver.save(step, model, results[args.metric_name], device)\n",
    "#                     ema.resume(model)\n",
    "\n",
    "#                     # Log to console\n",
    "#                     results_str = ', '.join('{}: {:05.2f}'.format(k, v)\n",
    "#                                             for k, v in results.items())\n",
    "#                     log.info('Dev {}'.format(results_str))\n",
    "\n",
    "#                     # Log to TensorBoard\n",
    "#                     log.info('Visualizing in TensorBoard...')\n",
    "#                     for k, v in results.items():\n",
    "#                         tbx.add_scalar('dev/{}'.format(k), v, step)\n",
    "#                     util.visualize(tbx,\n",
    "#                                    pred_dict=pred_dict,\n",
    "#                                    eval_path=args.dev_eval_file,\n",
    "#                                    step=step,\n",
    "#                                    split='dev',\n",
    "#                                    num_visuals=args.num_visuals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [1:45:39<00:00,  1.29s/it, NLL=0.283, epoch=1]\n",
      " 75%|███████▍  | 3733/5000 [1:26:28<30:39,  1.45s/it, NLL=0.303, epoch=2]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-aad596905035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-b761e2fbfc46>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3]).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
