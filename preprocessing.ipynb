{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Key Phrase Extractor\n",
    "\n",
    "In this notebook we aim to realize the Bottom-Up Summarization Paper's extractor with BERT as the contextual embedding and see if we are able to extract phrases that maximizes the ROGUE scores. Our first goal in this project is to generate non-sensical summaries that maximizes the ROGUE score. Then, we aim to train an additional language model-like network to generate abstractive summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from rouge import Rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_STORY_DIR = os.path.join('data', 'cnn', 'stories')\n",
    "DM_STORY_DIR = os.path.join('data', 'dailymail', 'stories')\n",
    "\n",
    "CNN_STORY_TOKENIZED = os.path.join('data', 'cnn', 'stories-tokenized')\n",
    "DM_STORY_TOKENIZED = os.path.join('data', 'dailymail', 'stories-tokenized')\n",
    "\n",
    "SRC_JSON = os.path.join('data', 'src.pk')\n",
    "TGT_JSON = os.path.join('data', 'tgt.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = os.path.join('data', 'docs.pk')\n",
    "TAGS = os.path.join('data', 'tags.pk')\n",
    "TAGGED_SUMS = os.path.join('data', 'tagged_sums.pk')\n",
    "GOLD_SUMS = os.path.join('data', 'gold_sums.pk')\n",
    "IDX_TAGS = os.path.join('data', 'idx_tags.pk')\n",
    "IDS = os.path.join('data', 'idx.pk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We will first read in the files and process them into tokenized sentences and words, and separate out the source document and the abstract. Here, we heavily borrowed code from Pointer Generator code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [d for d in os.listdir(CNN_STORY_TOKENIZED)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_single_close_quote = u'\\u2019' # unicode\n",
    "dm_double_close_quote = u'\\u201d'\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "def process_json(filename):\n",
    "    src, tgt = [], [] # a document is a list of list of words\n",
    "    highlight = False # highlights are always at the end of the document \n",
    "    f = open(filename, 'r')\n",
    "    parsed = json.load(f)\n",
    "    for sent in parsed['sentences']:\n",
    "        words = [word['word'] for word in sent['tokens']]\n",
    "        if words[-1] not in END_TOKENS:\n",
    "            words += ['.']\n",
    "        if words[0] == '@highlight':\n",
    "            highlight = True\n",
    "        elif highlight:\n",
    "            tgt += [words]\n",
    "        else:\n",
    "            src += [words]\n",
    "    return src, tgt\n",
    "\n",
    "src, tgt = process_json(os.path.join(CNN_STORY_TOKENIZED, dirs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_in_src_vocab(src, tgt):\n",
    "    src_vocab = set()\n",
    "    for sent in src:\n",
    "        src_vocab |= set(sent)\n",
    "    count = 0\n",
    "    total_len = 0\n",
    "    for sent in tgt:\n",
    "        for word in sent:\n",
    "            if word in src_vocab:\n",
    "                count += 1\n",
    "            total_len += 1\n",
    "    return count / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_json(file_dir):\n",
    "    pool = Pool(processes=10)\n",
    "    srcs, tgts = [], []\n",
    "    percentages = []\n",
    "    file_paths = [os.path.join(file_dir, file_name) for file_name in os.listdir(file_dir)]\n",
    "    for tup in pool.imap_unordered(process_json, file_paths):\n",
    "        src, tgt = tup\n",
    "        srcs.append(src)\n",
    "        tgts.append(tgt)\n",
    "        percentages.append(percentage_in_src_vocab(src, tgt))\n",
    "    print(np.mean(percentages))\n",
    "    return srcs, tgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs_cnn, tgts_cnn = process_all_json(CNN_STORY_TOKENIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs_dm, tgts_dm = process_all_json(DM_STORY_TOKENIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt = srcs_cnn + srcs_dm, tgts_cnn + tgts_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(SRC_JSON, 'wb')\n",
    "pickle.dump(src, f)\n",
    "f.close()\n",
    "\n",
    "f = open(TGT_JSON, 'wb')\n",
    "pickle.dump(tgt, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(SRC_JSON, 'rb')\n",
    "src = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(TGT_JSON, 'rb')\n",
    "tgt = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess to BERT\n",
    "\n",
    "To use BERT, we must format our data into one that BERT is able to use. We also have to redefine the problem as a sequence tagging problem presented in the Bottom-Up paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', max_len=510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(doc, tgt):\n",
    "    \"\"\"\n",
    "    doc: a list of src tokens\n",
    "    tgt: a list of tgt tokens that we will look for in the doc\n",
    "    \"\"\"\n",
    "    if len(tgt) == 0:\n",
    "        print('zero sized tgt')\n",
    "        return None\n",
    "    vocab = set(tgt)\n",
    "    doc = np.array(doc)\n",
    "    tgt = np.array(tgt)\n",
    "\n",
    "    label = np.zeros(len(doc), dtype=bool)\n",
    "    ## The following tags all tokens present in both the source and target\n",
    "#     for i in range(len(doc)):\n",
    "#         if doc[i] in vocab:\n",
    "#             label[i] = 1\n",
    "    ## The following does the max tagging thingy the original paper did\n",
    "    l, r = 0, 0\n",
    "    while r < len(tgt):\n",
    "        old_idxs = []\n",
    "        idxs = [(i,i+1) for i, token in enumerate(doc) if token == tgt[r]]\n",
    "        while len(idxs) > 0 and r + 1 < len(tgt):\n",
    "            r += 1\n",
    "            old_idxs, idxs = idxs, []\n",
    "            for idx in old_idxs:\n",
    "                if idx[-1] < len(doc) and doc[idx[-1]] == tgt[r]:\n",
    "                    idxs.append((idx[0], idx[-1] + 1))\n",
    "        if len(idxs) > 0: ## we ran out of tgt\n",
    "            label[idxs[0][0]:idxs[0][-1]] = 1\n",
    "            break\n",
    "        elif len(old_idxs) > 0: ## we found longest seq\n",
    "            label[old_idxs[0][0]:old_idxs[0][-1]] = 1\n",
    "        else: ## this token does not exist\n",
    "            r += 1\n",
    "    idxs = []\n",
    "    for i in range(len(tgt)):\n",
    "        idxs.append(list(np.argwhere(doc == tgt[i]).flatten()))\n",
    "    return label, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_src_tgt(srcs, tgts, start_idx=0, end_idx=-1):\n",
    "    assert len(srcs) == len(tgts)\n",
    "    docs, tags = [], []\n",
    "    tagged_sum, gold_sum_bert, gold_sum_idxs = [], [], []\n",
    "    ranges = []\n",
    "    rn = range(len(srcs)) if end_idx == -1 else range(start_idx, end_idx)\n",
    "    for i in rn:\n",
    "        ## process src\n",
    "        sents = [' '.join(sent) + ' [SEP]' for sent in srcs[i]]\n",
    "        doc = ' '.join(['[CLS]'] + sents)\n",
    "        doc = tokenizer.tokenize(doc)[:510]\n",
    "\n",
    "        ## process tgt\n",
    "        tgt = ' '.join([' '.join(sent) for sent in tgts[i]])\n",
    "        tgt = tokenizer.tokenize(tgt)[:110]\n",
    "        label, idxs = tag(doc, tgt)\n",
    "        \n",
    "        ## generate tagged_summary for oracle rouge\n",
    "        tagged = []\n",
    "        for idx in idxs:\n",
    "            doc = np.array(doc)\n",
    "            if len(doc[idx]) > 0:\n",
    "                tagged.append(doc[idx][0])\n",
    "\n",
    "        ## Add both to list\n",
    "        docs.append(tokenizer.convert_tokens_to_ids(doc))\n",
    "        tags.append(label)\n",
    "        tagged_sum.append((' '.join(tagged)).replace(' ##', ''))\n",
    "        gold_sum_bert.append(' '.join(tgt).replace(' ##', ''))\n",
    "        gold_sum_idxs.append(idxs)\n",
    "        ranges.append(i)\n",
    "    return docs, tags, tagged_sum, gold_sum_bert, np.array(gold_sum_idxs), ranges\n",
    "docs, tags, tagged_sum, gold_sum_bert, gold_sum_idxs, ranges = process_src_tgt(src, tgt, 9, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sami al - hajj arrives home in sudan and is taken to hospital , network says . pakistani intelligence officers captured him in afghanistan in december 2001 . al - hajj was transferred to u . s . custody and held without charges or trial . al - jazeera said he was on an assignment when he was apprehended .']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_sum_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sami al - hajj home in sudan and is taken to hospital , network . pakistani intelligence officers captured him in afghanistan in december 2001 . al - hajj was to u . s . and held without or trial . al - jazeera said he was on an assignment he was .']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ranges(args):\n",
    "    return process_src_tgt(src, tgt, args[0], args[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 35\n",
    "pool = Pool(n)\n",
    "k = len(src)//n\n",
    "result = pool.map(process_ranges, [(start * k, (start+1) * k) for start in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_strictly_increasing(nested_sequence):\n",
    "    counter = 0\n",
    "    for sequence in nested_sequence:\n",
    "        for i in sequence:\n",
    "            if i != counter:\n",
    "                return False\n",
    "            counter += 1\n",
    "    return True\n",
    "check_strictly_increasing([tup[-1] for tup in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(lst, valid_ids):\n",
    "    return [lst[i] for i in valid_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, tags, tagged_sums, gold_sums_bert, gold_sums_idxs, ids = [], [], [], [], [], []\n",
    "for a, b, c, d, e, f in result:\n",
    "    valid_ids = [i for i in range(len(a)) if len(c[i]) > 0 and len(d[i]) > 0]\n",
    "    docs.extend(clean(a, valid_ids))\n",
    "    tags.extend(clean(b, valid_ids))\n",
    "    tagged_sums.extend(clean(c, valid_ids))\n",
    "    gold_sums_bert.extend(clean(d, valid_ids))\n",
    "    gold_sums_idxs.extend(clean(e, valid_ids))\n",
    "    ids.extend(clean(f, valid_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj, fname in zip([docs, tags, tagged_sums, gold_sums_bert, gold_sums_idxs, ids], \n",
    "                      [DOCS, TAGS, TAGGED_SUMS, GOLD_SUMS, IDX_TAGS, IDS]):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.8811618935491741, 'p': 0.9932318864311466, 'r': 0.8002525427630327}, 'rouge-2': {'f': 0.7636215390100871, 'p': 0.8396691890160917, 'r': 0.7053841629669394}, 'rouge-l': {'f': 0.8550852296513152, 'p': 0.993229688715533, 'r': 0.8002509720851069}}\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(tagged_sums, gold_sums_bert, avg=True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Model\n",
    "\n",
    "We have calculated the \"oracle\" score above, and now we would like to fit a model that accurately predicts the tags defined above.\n",
    "\n",
    "Later, we might change how the tags are defined and see if we can achieve better results than \"first occurance tagging\"\n",
    "\n",
    "We will split 90/5/5 with a 5k tiny dataset selected from the train set for faster development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, tags, tagged_sums, gold_sums_bert, gold_sums_idxs, ids = \\\n",
    "    [pickle.load(open(file_path, 'rb')) for file_path in [DOCS, TAGS, TAGGED_SUMS, GOLD_SUMS, IDX_TAGS, IDS]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {'rouge-1': {'f': 0.8811621861531733, 'p': 0.9932318101053599, 'r': 0.8002531077407594}, 'rouge-2': {'f': 0.763622203917866, 'p': 0.839669677462507, 'r': 0.7053849661328021}, 'rouge-l': {'f': 0.8550856135961422, 'p': 0.9932296123897465, 'r': 0.8002515370628336}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev_test, y_tags_train, y_tags_dev_test, y_decode_train, y_decode_dev_test, ids_train, ids_dev_test = \\\n",
    "        train_test_split(docs, tags, gold_sums_idxs, ids, test_size=0.1)\n",
    "X_dev, X_test, y_tags_dev, y_tags_test, y_decode_dev, y_decode_test, ids_dev, ids_test =\\\n",
    "        train_test_split(X_dev_test, y_tags_dev_test, y_decode_dev_test, ids_dev_test, test_size=0.5)\n",
    "X_tiny, y_tags_tiny, y_decode_tiny, ids_tiny = \\\n",
    "        X_train[:5000], y_tags_train[:5000], y_decode_train[:5000], ids_train[:5000]\n",
    "processed = dict()\n",
    "processed['train'] = {'X':X_train, 'y_tag':y_tags_train, 'y_decode':y_decode_train,\n",
    "        'ids':ids_train}\n",
    "processed['dev'] = {'X':X_dev, 'y_tag':y_tags_dev, 'y_decode':y_decode_dev,\n",
    "        'ids':ids_dev}\n",
    "processed['test'] = {'X':X_test, 'y_tag':y_tags_test, 'y_decode':y_decode_test,\n",
    "        'ids':ids_test}\n",
    "processed['tiny'] = {'X':X_tiny, 'y_tag':y_tags_tiny, 'y_decode':y_decode_tiny,\n",
    "        'ids':ids_tiny}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA = os.path.join('data', 'data.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSED_DATA, 'wb') as f:\n",
    "    pickle.dump(processed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSED_DATA, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tiny, y_tags_tiny, y_decode_tiny, ids_tiny = \\\n",
    "    data['tiny']['X'], data['tiny']['y_tag'], data['tiny']['y_decode'], data['tiny']['ids'],\n",
    "super_tiny = {'tiny':{'X':X_tiny[:10], 'y_tag':y_tags_tiny[:10], 'y_decode':y_decode_tiny[:10],\n",
    "        'ids':ids_tiny[:10]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPER_TINY = os.path.join('data', 'super_tiny.pk')\n",
    "with open(SUPER_TINY, 'wb') as f:\n",
    "    pickle.dump(super_tiny, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', max_len=510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] a grieving dental nurse who piled on the pounds grieving the loss of her best friend has shed more than four stone to turn herself into a muscle - bound beauty queen . [SEP] sarah jayne maher , 26 , from denton , greater manchester , ballooned to 13 stone 12 lbs and suffered severe depression after the loss of her best friend simone hill in a car crash in january 2011 . [SEP] but after seeing a picture of herself looking overweight on a night out with friends , the 5ft 3in blonde shed the pounds and bulked up into a body - building beauty queen . [SEP] sarah jane maher piled on the pounds after the death of her best friend , but has since gone on to lose 4st and now competes as a body building beauty queen - lrb - right - rrb - . [SEP] and she is set to compete as miss manchester in the miss galaxy uk pageant on february 8 . [SEP] sarah was left severely depressed after the death of simone , who was tragically killed in a car crash in reddish , greater manchester , while travelling to meet up on her 18th birthday . [SEP] the horse - mad pair had always been active as they groomed and cared for sarah ' s first horse , rusty , after he was given to her as an eighth birthday present . [SEP] sarah said : ` i was athletic as a kid and i was always with my horses and simone , competing in horse shows three times a month . [SEP] sarah was inspired to go into body building after a chance meeting with female weight - lifter kizzy vaines . [SEP] sarah was determined to lose the weight after seeing a photo of her on a night out - lrb - left - rrb - she has since gone on to win the north wales miss bikini trophy - lrb - right - rrb - . [SEP] ` but once she died , i just wanted to get rid of any reminder of her . [SEP] the memories were just too painful . [SEP] ` i realise now that it was a mistake . [SEP] i even had to give my horse , george , to a friend to look after for me . [SEP] ` i was so depressed . [SEP] i did n ' t want to go out , i did n ' t want to do anything . [SEP] all i did was stay in and eat . ' [SEP] sarah binged on takeaways every night and scoffed crisps and chocolate for 18 months . [SEP] but after seeing a picture of herself on a rare night out in york , she vowed to turn her life around and hit the weights . [SEP] before the tragic death\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenizer.convert_ids_to_tokens(super_tiny['tiny']['X'][0])).replace(' ##', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Collections must contain at least 1 sentence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2a410fc4e2bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrouge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRouge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrouge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'world dafsd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' adfasdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'world fdaskf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/rouge/rouge.py\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, hyps, refs, avg, ignore_empty)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_avg_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/rouge/rouge.py\u001b[0m in \u001b[0;36m_get_avg_scores\u001b[0;34m(self, hyps, refs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRouge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAVAILABLE_METRICS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/rouge/rouge.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(hyp, ref)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mDEFAULT_METRICS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"rouge-1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rouge-2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rouge-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     AVAILABLE_METRICS = {\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;34m\"rouge-1\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouge_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;34m\"rouge-2\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrouge_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouge_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;34m\"rouge-l\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/rouge/rouge_score.py\u001b[0m in \u001b[0;36mrouge_n\u001b[0;34m(evaluated_sentences, reference_sentences, n)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \"\"\"\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluated_sentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_sentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Collections must contain at least 1 sentence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mevaluated_ngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_word_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluated_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Collections must contain at least 1 sentence."
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "rouge.get_scores(['.', 'world dafsd'], [' adfasdf', 'world fdaskf'], avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sarah jane maher was after her friend died in a car crash . the 26 - - a , on for . after meeting body , sarah was determined to lose weight . she has now four stone and competes in beauty pageants .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = []\n",
    "for idx in super_tiny['tiny']['y_decode'][0]:\n",
    "    doc = np.array(super_tiny['tiny']['X'][0])\n",
    "    if len(doc[idx]) > 0:\n",
    "        tagged.append(doc[idx][0])\n",
    "' '.join(tokenizer.convert_ids_to_tokens(tagged)).replace(' ##', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a on the her friend four stone - beauty . , 26 and after in a car crash body sarah jane maher now competes pageant was for meeting weight sarah was determined to lose she has dieds'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = np.array(super_tiny['tiny']['X'][0])\n",
    "tagged = doc[np.array(super_tiny['tiny']['y_tag'][0])]\n",
    "' '.join(tokenizer.convert_ids_to_tokens(tagged)).replace(' ##', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative model (Unused)\n",
    "\n",
    "To train a generative model, we need to process the Glove embeddings.\n",
    "\n",
    "NO MORE GLOVE!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "def build_vocab(srcs):\n",
    "    vocab = {SENTENCE_START, SENTENCE_END}\n",
    "    for src in srcs:\n",
    "        # src is a list of list of words\n",
    "        for sent in src:\n",
    "            vocab |= set([word.lower() for word in sent])\n",
    "    return {word:i for i, word in enumerate(vocab)}\n",
    "vocab = build_vocab(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_HOME = os.path.join('data', 'glove.840B.300d.txt')\n",
    "def glove2dict(src_filename, model_vocab):\n",
    "    \"\"\"GloVe Reader.\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_filename : str\n",
    "        Full path to the GloVe file to be processed.\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping words to their GloVe vectors.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    with open(src_filename, 'r', newline=\"\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                line = next(f)\n",
    "                line = line.strip().split()\n",
    "                line[0] = line[0].lower()\n",
    "                if line[0] in model_vocab:\n",
    "                    data[line[0]] = np.array(line[1: ], dtype=np.float)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            except:\n",
    "                pass\n",
    "    return data\n",
    "glove = glove2dict(GLOVE_HOME, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocab = {word:i for i, word in enumerate(glove.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab) - len(glove_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_vocab = set()\n",
    "for t in tgt:\n",
    "    for sent in t:\n",
    "        for word in sent:\n",
    "            word = word.lower()\n",
    "            if word not in vocab:\n",
    "                not_in_vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vocab = {i:word for word, i in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([reverse_vocab[i] for i in gold_sum_idxs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
