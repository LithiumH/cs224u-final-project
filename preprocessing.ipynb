{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Key Phrase Extractor\n",
    "\n",
    "In this notebook we aim to realize the Bottom-Up Summarization Paper's extractor with BERT as the contextual embedding and see if we are able to extract phrases that maximizes the ROGUE scores. Our first goal in this project is to generate non-sensical summaries that maximizes the ROGUE score. Then, we aim to train an additional language model-like network to generate abstractive summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "\n",
    "from rouge import Rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_STORY_DIR = os.path.join('data', 'cnn', 'stories')\n",
    "DM_STORY_DIR = os.path.join('data', 'dailymail', 'stories')\n",
    "\n",
    "CNN_STORY_TOKENIZED = os.path.join('data', 'cnn', 'stories-tokenized')\n",
    "DM_STORY_TOKENIZED = os.path.join('data', 'dailymail', 'stories-tokenized')\n",
    "\n",
    "SRC_JSON = os.path.join('data', 'src.pk')\n",
    "TGT_JSON = os.path.join('data', 'tgt.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = os.path.join('data', 'docs.pk')\n",
    "TAGS = os.path.join('data', 'tags.pk')\n",
    "GOLD_SUMS = os.path.join('data', 'gold_sums.pk')\n",
    "IDS = os.path.join('data', 'idx.pk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We will first read in the files and process them into tokenized sentences and words, and separate out the source document and the abstract. Here, we heavily borrowed code from Pointer Generator code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [d for d in os.listdir(CNN_STORY_TOKENIZED)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_single_close_quote = u'\\u2019' # unicode\n",
    "dm_double_close_quote = u'\\u201d'\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "def process_json(filename):\n",
    "    src, tgt = [], [] # a document is a list of list of words\n",
    "    highlight = False # highlights are always at the end of the document \n",
    "    f = open(filename, 'r')\n",
    "    parsed = json.load(f)\n",
    "    for sent in parsed['sentences']:\n",
    "        words = [word['word'] for word in sent['tokens']]\n",
    "        if words[-1] not in END_TOKENS:\n",
    "            words += ['.']\n",
    "        if words[0] == '@highlight':\n",
    "            highlight = True\n",
    "        elif highlight:\n",
    "            tgt += [words]\n",
    "        else:\n",
    "            src += [words]\n",
    "    return src, tgt\n",
    "\n",
    "src, tgt = process_json(os.path.join(CNN_STORY_TOKENIZED, dirs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_in_src_vocab(src, tgt):\n",
    "    src_vocab = set()\n",
    "    for sent in src:\n",
    "        src_vocab |= set(sent)\n",
    "    count = 0\n",
    "    total_len = 0\n",
    "    for sent in tgt:\n",
    "        for word in sent:\n",
    "            if word in src_vocab:\n",
    "                count += 1\n",
    "            total_len += 1\n",
    "    return count / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_json(file_dir):\n",
    "    pool = Pool(processes=10)\n",
    "    srcs, tgts = [], []\n",
    "    percentages = []\n",
    "    file_paths = [os.path.join(file_dir, file_name) for file_name in os.listdir(file_dir)]\n",
    "    for tup in pool.imap_unordered(process_json, file_paths):\n",
    "        src, tgt = tup\n",
    "        srcs.append(src)\n",
    "        tgts.append(tgt)\n",
    "        percentages.append(percentage_in_src_vocab(src, tgt))\n",
    "    print(np.mean(percentages))\n",
    "    return srcs, tgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs_cnn, tgts_cnn = process_all_json(CNN_STORY_TOKENIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs_dm, tgts_dm = process_all_json(DM_STORY_TOKENIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt = srcs_cnn + srcs_dm, tgts_cnn + tgts_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(SRC_JSON, 'wb')\n",
    "pickle.dump(src, f)\n",
    "f.close()\n",
    "\n",
    "f = open(TGT_JSON, 'wb')\n",
    "pickle.dump(tgt, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(SRC_JSON, 'rb')\n",
    "src = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(TGT_JSON, 'rb')\n",
    "tgt = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess to BERT\n",
    "\n",
    "To use BERT, we must format our data into one that BERT is able to use. We also have to redefine the problem as a sequence tagging problem presented in the Bottom-Up paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', max_len=510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_dist(tup1, tup2):\n",
    "    \"\"\"\n",
    "    This function calculates the distance between 2 ranges.\n",
    "    \"\"\"\n",
    "    start1, end1 = min(tup1), max(tup1)\n",
    "    start2, end2 = min(tup2), max(tup2)\n",
    "    if start1 < start2 < end1 or start1 < end2 < end1: # overlap\n",
    "        return 0\n",
    "    return min(abs(end1 - start2), abs(end2 - start1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(doc, tgt):\n",
    "    \"\"\"\n",
    "    doc: a list of src tokens\n",
    "    tgt: a list of tgt tokens that we will look for in the doc\n",
    "    \n",
    "    returns:\n",
    "    decode_label: a list of size tgt (or less) denoting the positions of the tokens at each summ step\n",
    "    \"\"\"\n",
    "    if len(tgt) == 0:\n",
    "        print('zero sized tgt')\n",
    "        return None\n",
    "\n",
    "    decode_label = []\n",
    "    l, r, last_range = 0, 0, (0, 0) # last step is the index into the src where we chose last\n",
    "    while r < len(tgt):\n",
    "        old_idxs = []\n",
    "        idxs = [(i,i+1) for i, token in enumerate(doc) if token == tgt[r]]\n",
    "        while len(idxs) > 0: # found a match\n",
    "            r += 1\n",
    "            old_idxs, idxs = idxs, []\n",
    "            for start, end in old_idxs:\n",
    "                if end < len(doc) and r < len(tgt) and doc[end] == tgt[r]:\n",
    "                    idxs.append((start, end + 1))\n",
    "        idx_to_look = old_idxs if len(idxs) == 0 else idxs\n",
    "        if len(idx_to_look) == 0:\n",
    "            r += 1\n",
    "        else:\n",
    "            best_i = min(range(len(idx_to_look)), key=lambda i: range_dist(last_range, idx_to_look[i]))\n",
    "            last_range = idx_to_look[best_i]\n",
    "            decode_label.extend(list(range(last_range[0], last_range[1])))\n",
    "    return decode_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bert_tokens(sent):\n",
    "    return re.sub(r'( ##)|(\\[CLS\\] )|(\\s*\\[SEP\\])','', sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_src_tgt(srcs, tgts, start_idx=0, end_idx=-1):\n",
    "    assert len(srcs) == len(tgts)\n",
    "    docs, tags, gold_sums_bert, ranges = [], [], [], []\n",
    "    rn = range(len(srcs)) if end_idx == -1 else range(start_idx, end_idx)\n",
    "    for i in rn:\n",
    "        ## process src\n",
    "        doc = '[CLS] ' + ' '.join(' '.join(sent) + ' [SEP]' for sent in srcs[i])\n",
    "        doc = tokenizer.tokenize(doc)[:510]\n",
    "\n",
    "        ## process tgt\n",
    "        tgt = '[CLS] ' + ' '.join(' '.join(sent) + ' [SEP]' for sent in tgts[i])\n",
    "#         tgt = ' '.join([' '.join(sent) for sent in tgts[i]]) ## Tried adding SEP and CLS, doesn't really work..\n",
    "        tgt = tokenizer.tokenize(tgt)[:110]\n",
    "        label = tag(doc, tgt)\n",
    "\n",
    "        ## Add both to list\n",
    "        docs.append(tokenizer.convert_tokens_to_ids(doc))\n",
    "        tags.append(label)\n",
    "        gold_sums_bert.append(remove_bert_tokens(' '.join(tgt)))\n",
    "        ranges.append(i)\n",
    "    return docs, tags, gold_sums_bert, ranges\n",
    "docs, tags, gold_sums_bert, ranges = process_src_tgt(src, tgt, 9, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_sums_bert[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(tokenizer.convert_ids_to_tokens([docs[0][i] for i in tags[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ranges(args):\n",
    "    return process_src_tgt(src, tgt, args[0], args[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 35\n",
    "pool = Pool(n)\n",
    "k = len(src)//n\n",
    "result = pool.map(process_ranges, [(start * k, (start+1) * k) for start in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_strictly_increasing(nested_sequence):\n",
    "    counter = 0\n",
    "    for sequence in nested_sequence:\n",
    "        for i in sequence:\n",
    "            if i != counter:\n",
    "                return False\n",
    "            counter += 1\n",
    "    return True\n",
    "check_strictly_increasing([tup[-1] for tup in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(lst, valid_ids):\n",
    "    return [lst[i] for i in valid_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, tags, gold_sums_bert, ids = [], [], [], []\n",
    "for a, b, c, d in result:\n",
    "    valid_ids = [i for i in range(len(a)) if len(b[i]) > 0 and len(c[i]) > 0]\n",
    "    docs.extend(clean(a, valid_ids))\n",
    "    tags.extend(clean(b, valid_ids))\n",
    "    gold_sums_bert.extend(clean(c, valid_ids))\n",
    "    ids.extend(clean(d, valid_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rafael nadal beats leonardo mayer in straight sets . andy murray locked in five set struggle when play halted . gael monfils wins epic five - setter against fabio fognini . sloane stephens to face simona halep in last 16 .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_sums_bert[2039]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] rafael nad ##al leonardo mayer in straight sets . [SEP] andy murray locked in five set when play halted . [SEP] gael mon ##fi ##ls five - set against fabio fog ##nin ##i . [SEP] sloane stephens to face simon ##a hale ##p in last . [SEP]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenizer.convert_ids_to_tokens([docs[2039][i] for i in tags[2039]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to data/docs.pk\n",
      "saving to data/tags.pk\n",
      "saving to data/gold_sums.pk\n",
      "saving to data/idx.pk\n"
     ]
    }
   ],
   "source": [
    "for obj, fname in zip([docs, tags, gold_sums_bert, ids], \n",
    "                      [DOCS, TAGS, GOLD_SUMS,      IDS]):\n",
    "    with open(fname, 'wb') as f:\n",
    "        print(\"saving to %s\" % fname)\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Rouge\n",
    "\n",
    "The oracle rouge score can be calculated here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sums = [remove_bert_tokens(' '.join(tokenizer.convert_ids_to_tokens([docs[i][j] for j in tags[i]]))) \\\n",
    "               for i in range(len(docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'f': 0.881164808423798, 'p': 0.9928848305179249, 'r': 0.8004520423694316}, 'rouge-2': {'f': 0.7637302213937581, 'p': 0.8396110056078727, 'r': 0.7056012489332818}, 'rouge-l': {'f': 0.8552061121823076, 'p': 0.9928827635932947, 'r': 0.8004505631941881}}\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(tagged_sums, gold_sums_bert, avg=True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'rouge-1': {'f': 0.881164808423798, 'p': 0.9928848305179249, 'r': 0.8004520423694316}, 'rouge-2': {'f': 0.7637302213937581, 'p': 0.8396110056078727, 'r': 0.7056012489332818}, 'rouge-l': {'f': 0.8552061121823076, 'p': 0.9928827635932947, 'r': 0.8004505631941881}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Model\n",
    "\n",
    "We have calculated the \"oracle\" score above, and now we would like to fit a model that accurately predicts the tags defined above.\n",
    "\n",
    "Later, we might change how the tags are defined and see if we can achieve better results than \"first occurance tagging\"\n",
    "\n",
    "We will split 90/5/5 with a 5k tiny dataset selected from the train set for faster development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, tags, gold_sums_bert, ids = \\\n",
    "    [pickle.load(open(file_path, 'rb')) for file_path in [DOCS, TAGS, GOLD_SUMS, IDS]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev_test, y_train, y_dev_test, gold_train, gold_dev_test, ids_train, ids_dev_test = \\\n",
    "        train_test_split(docs, tags, gold_sums_bert, ids, test_size=0.1)\n",
    "X_dev, X_test, y_dev, y_test, gold_dev, gold_test, ids_dev, ids_test = \\\n",
    "        train_test_split(X_dev_test, y_dev_test, gold_dev_test, ids_dev_test, test_size=0.5)\n",
    "X_tiny, y_tiny, gold_tiny, ids_tiny = \\\n",
    "        X_train[:500], y_train[:500], gold_train[:500], ids_train[:500]\n",
    "data = dict()\n",
    "data['train'] = {'X': X_train, 'y': y_train, 'gold': gold_train, 'ids':ids_train}\n",
    "data['dev'] = {'X': X_dev, 'y': y_dev, 'gold': gold_dev, 'ids':ids_dev}\n",
    "data['test'] = {'X': X_test, 'y': y_test, 'gold': gold_test, 'ids':ids_test}\n",
    "data['tiny'] = {'X': X_tiny, 'y': y_tiny, 'gold': gold_tiny, 'ids':ids_tiny}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA = os.path.join('data', 'data.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSED_DATA, 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSED_DATA, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tiny, y_tags_tiny, gold_tiny, ids_tiny = \\\n",
    "    data['tiny']['X'], data['tiny']['y'], data['tiny']['gold'], data['tiny']['ids'],\n",
    "super_tiny = {'tiny':{'X':X_tiny[:10], 'y':y_tags_tiny[:10], 'gold':gold_tiny[:10],\n",
    "        'ids':ids_tiny[:10]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPER_TINY = os.path.join('data', 'super_tiny.pk')\n",
    "with open(SUPER_TINY, 'wb') as f:\n",
    "    pickle.dump(super_tiny, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe (NOT USED)\n",
    "\n",
    "Turns out BERT is too heavy weight and instead we would try to use GloVe + LSTM instead. We will first process the glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tag(doc, tgt):\n",
    "#     \"\"\"\n",
    "#     doc: a list of src tokens\n",
    "#     tgt: a list of tgt tokens that we will look for in the doc\n",
    "#     \"\"\"\n",
    "#     if len(tgt) == 0:\n",
    "#         print('zero sized tgt')\n",
    "#         return None\n",
    "#     vocab = set(tgt)\n",
    "#     doc = np.array(doc)\n",
    "#     tgt = np.array(tgt)\n",
    "\n",
    "#     label = np.zeros(len(doc), dtype=bool)\n",
    "    \n",
    "#     ## The following tags all tokens present in both the source and target\n",
    "# #     for i in range(len(doc)):\n",
    "# #         if doc[i] in vocab:\n",
    "# #             label[i] = 1\n",
    "#     ## The following does the max tagging thingy the original paper did\n",
    "#     l, r = 0, 0\n",
    "#     while r < len(tgt):\n",
    "#         old_idxs = []\n",
    "#         idxs = [(i,i+1) for i, token in enumerate(doc) if token == tgt[r]]\n",
    "#         while len(idxs) > 0 and r + 1 < len(tgt):\n",
    "#             r += 1\n",
    "#             old_idxs, idxs = idxs, []\n",
    "#             for idx in old_idxs:\n",
    "#                 if idx[-1] < len(doc) and doc[idx[-1]] == tgt[r]:\n",
    "#                     idxs.append((idx[0], idx[-1] + 1))\n",
    "#         if len(idxs) > 0: ## we ran out of tgt\n",
    "#             label[idxs[0][0]:idxs[0][-1]] = 1\n",
    "#             break\n",
    "#         elif len(old_idxs) > 0: ## we found longest seq\n",
    "#             label[old_idxs[0][0]:old_idxs[0][-1]] = 1\n",
    "#         else: ## this token does not exist\n",
    "#             r += 1\n",
    "#     idxs = []\n",
    "    \n",
    "#     return label, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = {word for doc in src for sent in doc for word in sent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(src_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE = 'data-decode'\n",
    "GLOVE_HOME = os.path.join(DATA_BASE, 'glove.840B.300d.txt')\n",
    "def glove2dict(src_filename, model_vocab):\n",
    "    \"\"\"GloVe Reader.\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_filename : str\n",
    "        Full path to the GloVe file to be processed.\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping words to their GloVe vectors.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    original_len, src_only_len = 0, 0\n",
    "    with open(src_filename, 'r', newline=\"\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                line = next(f)\n",
    "                line = line.strip().split()\n",
    "                line[0] = line[0].lower()\n",
    "                if line[0] in src_vocab:\n",
    "                    data[line[0]] = np.array(line[1: ], dtype=np.float)\n",
    "                    src_only_len += 1\n",
    "                original_len += 1\n",
    "            except StopIteration:\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "            except:\n",
    "                pass\n",
    "    return data\n",
    "glove = glove2dict(GLOVE_HOME, src_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "def build_vocab(srcs):\n",
    "    vocab = {SENTENCE_START, SENTENCE_END}\n",
    "    for src in srcs:\n",
    "        # src is a list of list of words\n",
    "        for sent in src:\n",
    "            vocab |= set([word.lower() for word in sent])\n",
    "    return {word:i for i, word in enumerate(vocab)}\n",
    "vocab = build_vocab(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocab = {word:i for i, word in enumerate(glove.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab) - len(glove_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_vocab = set()\n",
    "for t in tgt:\n",
    "    for sent in t:\n",
    "        for word in sent:\n",
    "            word = word.lower()\n",
    "            if word not in vocab:\n",
    "                not_in_vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vocab = {i:word for word, i in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([reverse_vocab[i] for i in gold_sum_idxs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
